image: timescale/promscale
# number of connector pods to spawn
replicaCount: 1

# Arguments that will be passed onto deployment pods
# The list of available cli flags is available at
# https://github.com/timescale/promscale/blob/master/docs/cli.md

# For example, to activate HA, bump the replicaCount and set those arguments:
# - -leader-election-pg-advisory-lock-id=1
# - -leader-election-pg-advisory-lock-prometheus-timeout=6s
# More info about HA: https://github.com/timescale/promscale/blob/master/docs/high-avaliability/prometheus-HA.md
args: []

# connection options to connect to a target db
connection:
  # user used to connect to TimescaleDB
  user: postgres
  password:
    # the template for generating the name of
    # a Secret object containing the password to
    # connect to TimescaleDB. The password should
    # be indexed by the user name (connection.user)
    secretTemplate: "{{ .Release.Name }}-timescaledb-passwords"
  host:
    # the template for generating the database host
    # location
    # for a hardcoded host name from another release, set:
    #   nameTemplate: "{{ already-deployed-timescale.default.svc.cluster.local }}"
    # for a host name of a timescaledb instance
    # deployed in the same release (without a cluster override) set:
    #   nameTemplate: {{ .Release.Name }}.{{ .Release.Namespace }}.svc.cluster.local
    nameTemplate: "{{ .Release.Name }}.{{ .Release.Namespace }}.svc.cluster.local"
  port: 5432
  sslMode: require
  # database name in which to store the metrics
  # must be created before start
  dbName: timescale

# Prometheus annotations to configure scraping metrics from the connector
prometheus:
  enabled: true
  # Using the predefined annotations from the Prometheus helm chart:
  # https://hub.helm.sh/charts/stable/prometheus
  annotations:
    prometheus.io/scrape: 'true'
    prometheus.io/port: '9201'
    prometheus.io/path: '/metrics'


# settings for the service to be created that will expose
# the promscale deployment
service:
  port: 9201
  loadBalancer:
    # If not enabled, we still expose the connector using a so called Headless Service
    # https://kubernetes.io/docs/concepts/services-networking/service/#headless-services
    enabled: true
    # Read more about the AWS annotations here:
    # https://kubernetes.io/docs/concepts/cluster-administration/cloud-providers/#aws
    # https://docs.aws.amazon.com/eks/latest/userguide/load-balancing.html
    annotations:
      # Setting idle-timeout to the maximum allowed value
      service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout: "4000"

      # service.beta.kubernetes.io/aws-load-balancer-type: nlb            # Use an NLB instead of ELB
      # service.beta.kubernetes.io/aws-load-balancer-internal: 0.0.0.0/0  # Internal Load Balancer

# settings for the drop-chunk CronJob that deletes data outside of
# the retention period
dropChunk:
  schedule: "0,30 * * * *"

# set your own limits
resources: {}
# https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#nodeselector
nodeSelector: {}
